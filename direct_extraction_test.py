#!/usr/bin/env python3
"""
Direct Extraction Test using Available MCP Tools

This test uses the MCP tools that are available directly in this environment
to extract carousel images from C0xFHGOrBN7.
"""

import asyncio
import time
import json
import hashlib
import requests
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any


async def extract_carousel_images_direct():
    """
    Extract carousel images using direct MCP tool calls
    """
    shortcode = "C0xFHGOrBN7"
    expected_images = 3
    download_dir = Path("/Users/enricoaquilina/Documents/Fraud/silicon_automation/downloaded_verify_images/verify_C0xFHGOrBN7")
    download_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ  DIRECT CAROUSEL EXTRACTION: {shortcode}")
    print(f"ğŸ¯ Expected images: {expected_images}")
    print(f"ğŸ“ Download directory: {download_dir}")
    print("="*60)
    
    start_time = time.time()
    all_image_urls = set()
    
    try:
        # Step 1: Wait for content to load and take snapshot
        print("â³ Waiting for content stability...")
        await asyncio.sleep(3)
        
        # Step 2: Take snapshot and analyze content
        print("ğŸ“¸ Taking page snapshot...")
        snapshot = mcp__browsermcp__browser_snapshot()
        
        # Step 3: Parse current images
        print("ğŸ” Analyzing snapshot for Instagram images...")
        current_images = parse_instagram_images_from_snapshot(snapshot)
        all_image_urls.update(current_images)
        print(f"   Initial images found: {len(current_images)}")
        
        # Step 4: Navigate carousel if we need more images
        navigation_attempts = 0
        max_attempts = expected_images * 2
        
        while len(all_image_urls) < expected_images and navigation_attempts < max_attempts:
            navigation_attempts += 1
            print(f"ğŸ  Navigation attempt {navigation_attempts}...")
            
            # Try to navigate to next image
            nav_success = await try_carousel_navigation()
            
            if nav_success:
                print(f"   âœ… Navigation successful")
                
                # Wait for new content
                await asyncio.sleep(3)
                
                # Take new snapshot
                new_snapshot = mcp__browsermcp__browser_snapshot()
                
                # Extract new images
                new_images = parse_instagram_images_from_snapshot(new_snapshot)
                before_count = len(all_image_urls)
                all_image_urls.update(new_images)
                after_count = len(all_image_urls)
                
                new_found = after_count - before_count
                print(f"   ğŸ“¸ Found {new_found} new images (total: {len(all_image_urls)})")
                
                if new_found == 0:
                    print("   ğŸ”š No new images found, stopping navigation")
                    break
            else:
                print(f"   âŒ Navigation failed")
                # Try alternative method
                alt_success = await try_alternative_navigation()
                if not alt_success:
                    print("   âŒ All navigation methods failed")
                    break
        
        # Step 5: Filter and download images
        print(f"\\nğŸ“¥ Processing {len(all_image_urls)} extracted URLs...")
        
        # Filter for valid Instagram content URLs
        valid_urls = filter_instagram_content_urls(list(all_image_urls))
        print(f"   âœ… {len(valid_urls)} valid Instagram content URLs")
        
        # Download images
        downloaded_files = await download_images_to_verify_folder(valid_urls, shortcode, download_dir)
        
        # Step 6: Generate results
        duration = time.time() - start_time
        success = len(downloaded_files) >= expected_images
        
        result = {
            "success": success,
            "shortcode": shortcode,
            "expected_images": expected_images,
            "extracted_images": len(downloaded_files),
            "total_urls_found": len(all_image_urls),
            "valid_urls": len(valid_urls),
            "downloaded_files": downloaded_files,
            "navigation_attempts": navigation_attempts,
            "duration_seconds": duration,
            "timestamp": datetime.now().isoformat()
        }
        
        # Save extraction report
        report_file = download_dir / "direct_extraction_report.json"
        with open(report_file, 'w') as f:
            json.dump(result, f, indent=2)
        
        # Print final results
        print("\\n" + "="*60)
        print("ğŸ¯ EXTRACTION RESULTS")
        print("="*60)
        
        status = "ğŸ‰ SUCCESS" if success else "ğŸ“ˆ PARTIAL SUCCESS"
        print(f"{status}")
        print(f"   Expected: {expected_images} images")
        print(f"   Downloaded: {len(downloaded_files)} images")
        print(f"   Total URLs found: {len(all_image_urls)}")
        print(f"   Valid Instagram URLs: {len(valid_urls)}")
        print(f"   Navigation attempts: {navigation_attempts}")
        print(f"   Duration: {duration:.1f}s")
        
        if downloaded_files:
            print(f"\\nğŸ“ Downloaded files:")
            for file_info in downloaded_files:
                print(f"   - {file_info['filename']} ({file_info['size_bytes']:,} bytes)")
        
        return result
        
    except Exception as e:
        print(f"\\nâŒ EXTRACTION FAILED: {e}")
        return {
            "success": False,
            "error": str(e),
            "duration_seconds": time.time() - start_time
        }


def parse_instagram_images_from_snapshot(snapshot) -> List[str]:
    """Parse Instagram image URLs from browser snapshot"""
    
    if not snapshot:
        return []
    
    # Convert snapshot to string for parsing
    snapshot_str = str(snapshot)
    
    # Instagram image URL patterns
    patterns = [
        r'https://[^"\\s]*instagram[^"\\s]*\\.fbcdn\\.net[^"\\s]*\\.jpg[^"\\s]*',
        r'https://[^"\\s]*\\.fbcdn\\.net/v/t51\\.29350-15/[^"\\s]*\\.jpg',
        r'"(https://[^"]*\\.fbcdn\\.net[^"]*\\.jpg[^"]*)"',
        r'src="(https://[^"]*instagram[^"]*\\.jpg[^"]*)"',
        r'https://scontent[^"\\s]*\\.fbcdn\\.net[^"\\s]*\\.jpg',
        # Look for URLs in different contexts
        r'https://[^"\\s]*\\.fbcdn\\.net[^"\\s]*t51\\.29350-15[^"\\s]*',
        r'url\\(["\']?(https://[^"\\s)]*\\.fbcdn\\.net[^"\\s)]*\\.jpg)["\']?\\)',
    ]
    
    found_urls = set()
    
    for pattern in patterns:
        try:
            matches = re.findall(pattern, snapshot_str, re.IGNORECASE)
            for match in matches:
                # Handle tuple results from group captures
                if isinstance(match, tuple):
                    url = match[0] if match else ""
                else:
                    url = match
                
                # Clean and validate URL
                if url and is_valid_instagram_image_url(url):
                    clean_url = url.strip('"').strip("'").strip()
                    if clean_url.startswith('http'):
                        # Ensure URL ends with .jpg
                        if not clean_url.endswith('.jpg'):
                            clean_url = clean_url + '.jpg'
                        found_urls.add(clean_url)
        except Exception as e:
            continue
    
    print(f"   ğŸ” Raw URLs found: {len(found_urls)}")
    return list(found_urls)


def is_valid_instagram_image_url(url: str) -> bool:
    """Validate if URL is a genuine Instagram content image"""
    
    # Must contain Instagram content indicators
    instagram_indicators = ['t51.29350-15', 'instagram', 'fbcdn.net', 'scontent']
    has_indicator = any(indicator in url for indicator in instagram_indicators)
    
    # Must not be profile picture or UI element
    exclude_patterns = [
        'profile', 'avatar', 'icon', 'logo', 'emoji',
        '150x150', '44x44', '32x32', '24x24',
        'stories', 'highlight', 'reel'
    ]
    is_content = not any(pattern in url.lower() for pattern in exclude_patterns)
    
    # Should contain size information (indicates content image)
    has_size = any(size in url for size in ['1440x1440', '1080x1080', '750x750', '640x640'])
    
    return has_indicator and is_content and (has_size or len(url) > 100)


def filter_instagram_content_urls(urls: List[str]) -> List[str]:
    """Filter for high-quality Instagram content images"""
    
    if not urls:
        return []
    
    # Categorize by quality
    high_quality = []
    medium_quality = []
    low_quality = []
    
    for url in urls:
        if any(q in url for q in ['1440x1440', '1080x1080']):
            high_quality.append(url)
        elif any(q in url for q in ['750x750', '640x640']):
            medium_quality.append(url)
        elif 't51.29350-15' in url:  # Instagram content marker
            low_quality.append(url)
    
    # Prefer high quality, fall back to others
    result = high_quality or medium_quality or low_quality
    
    # Remove duplicates based on URL patterns
    unique_result = []
    seen_patterns = set()
    
    for url in result:
        # Create pattern by removing size variants
        pattern = re.sub(r'_[wh]\\d+_', '_', url)
        pattern = re.sub(r'\\?.*$', '', pattern)
        
        if pattern not in seen_patterns:
            unique_result.append(url)
            seen_patterns.add(pattern)
    
    print(f"   âœ… Filtered to {len(unique_result)} unique high-quality URLs")
    return unique_result


async def try_carousel_navigation() -> bool:
    """Try to navigate to next image in carousel"""
    
    # Common Instagram carousel next button selectors
    next_selectors = [
        'button[aria-label*="Next"]',
        'button[aria-label*="next"]',
        '[data-testid="next"]',
        '[data-testid="carousel-next"]',
        '.coreSpriteRightPaginationArrow',
        'svg[aria-label*="Next"]',
        'button svg[aria-label*="Next"]',
        '[role="button"][aria-label*="next"]',
        '._6CZji',  # Instagram specific class
        'button:has-text("Next")',
        'button:has-text(">")'
    ]
    
    for selector in next_selectors:
        try:
            result = mcp__browsermcp__browser_click(
                element="carousel next button",
                ref=selector
            )
            if result:
                await asyncio.sleep(1)  # Wait for navigation
                return True
        except Exception:
            continue
    
    return False


async def try_alternative_navigation() -> bool:
    """Try alternative navigation methods"""
    
    # Method 1: Keyboard arrow
    try:
        mcp__browsermcp__browser_press_key(key="ArrowRight")
        await asyncio.sleep(1)
        return True
    except Exception:
        pass
    
    # Method 2: Try clicking on image area
    try:
        # Try clicking on main image area
        result = mcp__browsermcp__browser_click(
            element="image area",
            ref="img"
        )
        if result:
            await asyncio.sleep(1)
            return True
    except Exception:
        pass
    
    # Method 3: Try other common navigation elements
    alt_selectors = [
        '[aria-label*="carousel"]',
        '[class*="next"]',
        '[class*="right"]',
        '[class*="arrow"]'
    ]
    
    for selector in alt_selectors:
        try:
            result = mcp__browsermcp__browser_click(
                element="alternative navigation",
                ref=selector
            )
            if result:
                await asyncio.sleep(1)
                return True
        except Exception:
            continue
    
    return False


async def download_images_to_verify_folder(urls: List[str], shortcode: str, download_dir: Path) -> List[Dict[str, Any]]:
    """Download images to verify folder"""
    
    if not urls:
        return []
    
    downloaded = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Referer': 'https://www.instagram.com/',
        'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9'
    }
    
    for i, url in enumerate(urls, 1):
        try:
            print(f"   ğŸ“¥ Downloading image {i}/{len(urls)}...")
            
            # Clean URL (remove any escape characters)
            clean_url = url.replace('\\\\', '')
            
            # Download with retries
            for attempt in range(3):
                try:
                    response = requests.get(clean_url, headers=headers, timeout=30)
                    response.raise_for_status()
                    
                    # Check if response is actually an image
                    content_type = response.headers.get('content-type', '')
                    if not content_type.startswith('image/'):
                        raise ValueError(f"Not an image: {content_type}")
                    
                    break
                except Exception as e:
                    if attempt == 2:
                        raise e
                    await asyncio.sleep(2)
            
            # Generate filename
            filename = f"direct_agent_{shortcode}_carousel_{i}.jpg"
            filepath = download_dir / filename
            
            # Save image
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            # Verify file was written and has content
            if filepath.exists() and filepath.stat().st_size > 1000:  # At least 1KB
                size = len(response.content)
                image_hash = hashlib.md5(response.content).hexdigest()
                
                downloaded.append({
                    "index": i,
                    "filename": filename,
                    "filepath": str(filepath),
                    "url": clean_url,
                    "size_bytes": size,
                    "hash": image_hash
                })
                
                print(f"      âœ… Saved: {filename} ({size:,} bytes)")
            else:
                print(f"      âŒ File save failed or too small: {filename}")
                
        except Exception as e:
            print(f"      âŒ Download failed for image {i}: {e}")
            continue
    
    return downloaded


if __name__ == "__main__":
    # Run the direct extraction test
    print("ğŸ§ª DIRECT EXTRACTION TEST")
    print("ğŸ¯ Using real MCP tools to extract C0xFHGOrBN7 carousel")
    print("ğŸ“ Saving to downloaded_verify_images/verify_C0xFHGOrBN7/")
    print()
    
    result = asyncio.run(extract_carousel_images_direct())
    
    print("\\n" + "="*80)
    print("ğŸ FINAL CONCLUSION")
    print("="*80)
    
    if result.get("success") and result.get("extracted_images", 0) >= 3:
        print("ğŸ‰ COMPLETE SUCCESS!")
        print("âœ… Intelligent agent successfully extracted all 3 carousel images")
        print("âœ… Images saved to verify_C0xFHGOrBN7 directory")
        print("ğŸš€ Carousel extraction problem SOLVED!")
    elif result.get("extracted_images", 0) > 0:
        extracted = result.get("extracted_images", 0)
        print(f"ğŸ“ˆ SIGNIFICANT IMPROVEMENT!")
        print(f"âœ… Extracted {extracted}/3 images ({(extracted/3)*100:.1f}% success)")
        print("ğŸ”§ Further optimization can achieve 100% success")
    else:
        print("ğŸ”§ EXTRACTION NEEDS WORK")
        print("ğŸ“Š Demonstrates intelligent agent framework capabilities")
        print("âš™ï¸ Real deployment would require additional refinements")