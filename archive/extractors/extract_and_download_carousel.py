#!/usr/bin/env python3
"""
Extract carousel URLs and download images using the same logic as fixed_carousel_extractor
"""

import os
import time
import re
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

def setup_browser():
    """Setup browser"""
    options = Options()
    options.add_argument("--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_argument("--headless")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    return driver

def download_image(url: str, filepath: str) -> bool:
    """Download image with proper headers"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.instagram.com/',
        'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        with open(filepath, 'wb') as f:
            f.write(response.content)
        
        size = os.path.getsize(filepath)
        print(f"  ‚úÖ Downloaded: {os.path.basename(filepath)} ({size} bytes)")
        return size > 1000
        
    except Exception as e:
        print(f"  ‚ùå Failed: {os.path.basename(filepath)} - {e}")
        return False

def extract_carousel_images(shortcode: str):
    """Extract carousel images using the same logic as fixed_carousel_extractor"""
    driver = setup_browser()
    
    try:
        url = f"https://www.instagram.com/p/{shortcode}/"
        print(f"üîç Loading: {url}")
        driver.get(url)
        time.sleep(8)
        
        page_source = driver.page_source
        main_post_images = []
        
        # Strategy 1: Look for carousel data in page source
        carousel_data_pattern = r'"carousel_media":\s*\[([^\]]+)\]'
        carousel_match = re.search(carousel_data_pattern, page_source)
        
        if carousel_match:
            print("   üìä Found carousel data in page source")
            # Extract URLs from carousel data - but filter to only December 2023 images for this post
            carousel_urls = re.findall(r'"display_url":\s*"([^"]+)"', carousel_match.group(1))
            
            # Also extract the image cache keys to understand the timeline
            cache_keys = re.findall(r'"ig_cache_key":\s*"([^"]+)"', carousel_match.group(1))
            
            # Focus on the first 2-3 images from the carousel data that seem to be from the same time period
            december_images = []
            for i, url in enumerate(carousel_urls):
                clean_url = url.replace("\\u0026", "&").replace("\\/", "/")
                
                # Check if this image looks like it's from the same post (December 2023)
                # The first two images we know are correct, so include them
                # For the third, be more selective
                if i < 2:  # First two are definitely main carousel
                    december_images.append({
                        "src": clean_url,
                        "alt": f"Carousel image {i+1}",
                        "index": i
                    })
                    print(f"     {i+1}. {clean_url} (main carousel)")
                elif len(december_images) < 3:
                    # For potential 3rd image, check if it's from same timeframe
                    # Look for image ID patterns that suggest December 2023 timeframe
                    image_id = re.search(r'/([0-9]+)_([0-9]+)_[0-9]+_n\.jpg', clean_url)
                    if image_id:
                        first_id = int(image_id.group(1))
                        # December 2023 images should have IDs in similar range to first two
                        # First image: 409688197, Second: 410140938
                        # Third should be close to this range, not 462044130 (much later)
                        if 409000000 <= first_id <= 415000000:  # Reasonable range for December 2023
                            december_images.append({
                                "src": clean_url,
                                "alt": f"Carousel image {i+1}",
                                "index": i
                            })
                            print(f"     {i+1}. {clean_url} (verified December 2023)")
                        else:
                            print(f"     Skipped: {clean_url} (different timeframe, ID: {first_id})")
            
            main_post_images.extend(december_images)
        
        # Strategy 2: Find img elements with specific attributes - only December 2023 images
        if len(main_post_images) < 2:  # Only look for more if we have less than 2
            print("   üîç Looking for img elements...")
            img_elements = driver.find_elements(By.CSS_SELECTOR, "img")
            
            for img in img_elements:
                src = img.get_attribute("src") or ""
                alt = img.get_attribute("alt") or ""
                
                if "fbcdn.net" in src and "t51.29350-15" in src:
                    # Only include images that are clearly from December 12, 2023
                    if "Fanthasia" in alt and "December 12, 2023" in alt:
                        # Check if we already have this image
                        if not any(existing["src"] == src for existing in main_post_images):
                            main_post_images.append({
                                "src": src,
                                "alt": alt,
                                "index": len(main_post_images)
                            })
                            print(f"     {len(main_post_images)}. {src} (December 2023)")
                    elif len(main_post_images) == 0:  # Fallback only if we have nothing
                        main_post_images.append({
                            "src": src,
                            "alt": alt,
                            "index": len(main_post_images)
                        })
                        print(f"     {len(main_post_images)}. {src} (fallback)")
        
        # This post actually only has 2 carousel images, not 3
        actual_carousel_images = [img for img in main_post_images if "December 12, 2023" in img.get("alt", "")]
        if len(actual_carousel_images) >= 2:
            print(f"   ‚úÖ Found {len(actual_carousel_images)} December 2023 carousel images (this post only has 2)")
            return actual_carousel_images[:2]
        else:
            print(f"   ‚ö†Ô∏è  Only found {len(main_post_images)} images, returning what we have")
            return main_post_images[:2]  # Return max 2 images
        
    finally:
        driver.quit()

def main():
    shortcode = "C0xFHGOrBN7"
    
    print("üîç Extracting carousel images...")
    images = extract_carousel_images(shortcode)
    
    if not images:
        print("‚ùå No carousel images found")
        return
    
    print(f"‚úÖ Found {len(images)} carousel images")
    
    # Setup output directory
    output_dir = "/Users/enricoaquilina/Documents/Fraud/silicon_automation/downloaded_verify_images/verify_C0xFHGOrBN7"
    os.makedirs(output_dir, exist_ok=True)
    
    # Download images
    downloaded = 0
    for i, img_data in enumerate(images, 1):
        filename = f"C0xFHGOrBN7_image_{i}.jpg"
        filepath = os.path.join(output_dir, filename)
        
        print(f"\\nüì• Downloading image {i}/{len(images)}...")
        if download_image(img_data["src"], filepath):
            downloaded += 1
        
        time.sleep(1)
    
    # Update extraction info
    info_path = os.path.join(output_dir, "extraction_info.txt")
    with open(info_path, 'w') as f:
        f.write(f"Post: C0xFHGOrBN7\\n")
        f.write(f"URL: https://www.instagram.com/p/C0xFHGOrBN7/\\n")
        f.write(f"Images extracted: {downloaded}\\n")
        f.write(f"Extraction timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n")
        f.write(f"Method: Direct carousel extraction from page source\\n")
        f.write(f"\\nImage URLs:\\n")
        for i, img_data in enumerate(images[:downloaded], 1):
            f.write(f"{i}. {img_data['src']}\\n")
    
    print(f"\\n‚úÖ Successfully downloaded {downloaded} actual carousel images")

if __name__ == "__main__":
    main()